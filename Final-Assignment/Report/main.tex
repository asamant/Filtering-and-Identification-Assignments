%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Template: Project Titlepage Modified (v 0.1) by rcx
%
% Original Source: http://www.howtotex.com
% Date: February 2014
% 
% This is a title page template which be used for articles & reports.
% 
% This is the modified version of the original Latex template from
% aforementioned website.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{report}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url}
\usepackage{amsmath}
\usepackage{upgreek}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\begin{document}

\title{ \normalsize \textsc{SC42025 Filtering and Identification}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Turbulence Modeling for Adaptive Optics}}
		\HRule{2pt} \\ [0.5cm]
		\normalsize %\today
		\vspace*{5\baselineskip}}

%\date{}

\author{
		Aniket Ashwin Samant, Snehal Jauhri \\
		(Student IDs: 4838866, 4772202) \\ 
 \\ }

\maketitle
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% Section title formatting
\sectionfont{\scshape}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% BODY
%-------------------------------------------------------------------------------

\section*{Introduction}

This assignment deals with modeling an \textit{Adaptive Optics} (AO) system in which three different data-driven turbulence modeling methods are used to achieve optimal control performances, viz.
\begin{itemize}
	\item a random-walk process
	\item a Vector-Auto-Regressive model
	\item a stochastic state-space model
\end{itemize}

Each model has some questions associated with it, and we solve them in chronological sequence taking one model at a time.

\section*{1. Random Walk Model}

\subsection*{Question 1}

We know from the assignment's equation (2) that:

\begin{equation*}
	s_{o}(k) = G\phi(k) + e(k)
\end{equation*}

We have the values of the wavefront sensor data in open-loop, $s_{o}(k)$, and also the value of the matrix \textit{G}. To compute the value of $\hat\phi(k)$, given no prior information on it, we follow the linear least-squares approach:

First, we determine whether the matrix G is full-rank or not. We load the \textit{systemMatrices.mat} file which contains the matrix G, and then run the \textit{rank} command in MATLAB to get a rank value of \textbf{47}, which is less than \textit{min(number of rows, number of columns)} of G. Thus, we need to employ a linear least-squares method that doesn't assume the matrix G to be full-rank.

We know that there are multiple solutions to this problem, and for uniqueness we go with one such that the optimal solution, $\hat\phi(k)$, has a minimal 2-norm, thus leading to the original linear least-squares problem being reformulated as:

$\underset{\phi(k) \in \Gamma}{min} \, \|\phi(k)\|_{2}^{2}$ \space	with \space	$\Gamma = \left\{ \phi(k) : \phi(k) = arg \, \underset{z}{min} \| Gz - s_{o}(k)\|_{2}^{2}\right\}$
\\
By performing an SVD operation on the matrix G, we obtain:

\begin{equation*}
G = \begin{bmatrix}
U_{1} & U_{2}
\end{bmatrix}\begin{bmatrix}
\Sigma & 0 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
V_{1}^{T} \\
V_{2}^{T}
\end{bmatrix}
= U_{1}\Sigma V_{1}^{T}
\end{equation*}

Here, $\Sigma \in \Re^{47x47}$ is non-singular, by the definition of SVD. 

Now, let us define a partitioned vector,

\begin{equation*}
\begin{bmatrix}
\xi_{1} \\
\xi_{2}
\end{bmatrix}
=
\begin{bmatrix}
V_{1}^{T} \\
V_{2}^{T}
\end{bmatrix}z
\end{equation*}

Thus, our problem becomes:

$\underset{\xi_{1}}{min} \| U_{1}\Sigma\xi_{1}z - s_{o}(k)\|_{2}^{2}$

We get $\hat\xi_{1} = \Sigma^{-1}U_{1}^{T}s_{o}(k)$, since $\Sigma$ is a non-singular matrix. $\xi_{2}$ has no effect on the above expression and can be chosen arbitrarily. Thus, we get the optimal solution,
\begin{equation*}
\hat{z} = \begin{bmatrix}
V_{1} & V_{2}
\end{bmatrix}
\begin{bmatrix}
\hat\xi_{1} \\
\hat\xi_{2}
\end{bmatrix}
= V_{1}\Sigma^{-1}U_{1}^{T}s_{o}(k) + V_{2}\hat\xi_{2}
\end{equation*}

Since we're choosing a vector $\phi(k)$ with the smallest 2-norm, and $V_{1}^{T}V_{2} = 0$ we get:

$\|\phi(k)\|_{2}^{2} = \| V_{1}\Sigma^{-1}U_{1}^{T}s_{o}(k)\|_{2}^{2} + \| V_{2}\hat\xi_{2} \|_{2}^{2}$

As we're minimizing the norm, we take $\hat\xi_{2} = 0$ and we finally get the value of $\hat\phi(k)$ to be:
\begin{equation*}
\mathbf{\hat\phi(k) = V_{1}\Sigma^{-1}U_{1}^{T}s_{o}(k)}
\end{equation*}

\subsection*{Question 2}

We are provided with some prior information about the wavefront, viz.:

\begin{itemize}
	\item E[$\phi$(k)] = 0
	\item E[$\phi(k)\phi(k)^T$] = $C_{\phi}(0)$
	\item noise variance = $\sigma_{e}^{2}$
\end{itemize}
  
Based on equation (8) from the assignment, we approximate the value of $C_{\phi}(0)$ as:
\begin{equation}\label{eq:cPhi}
C_{\phi}(0) = \frac{1}{N}\sum_{i=1}^{N}\phi(i)\phi(i)^T
\end{equation}

We have the data necessary to formulate our problem of determining $\phi(k)$ as a stochastic linear least-squares problem, and hence we define our linear estimator $\tilde{\phi}(k)$ accordingly:

\begin{equation*}
	\tilde{\phi}(k) = \begin{bmatrix}
	M & N
	\end{bmatrix}
	\begin{bmatrix}
	s_{o}(k) \\
	E[\phi(k)]
	\end{bmatrix}
\end{equation*}  

such that E$\left[(\tilde{\phi}(k)- \phi(k))(\tilde{\phi}(k)- \phi(k))^T\right]$ is minimized and E[$\tilde{\phi}(k)$] = E[$\phi(k)$] = 0
\\\\
Thus, from the assignment's equation (2), based on $s_{o}(k)$'s expression, we can say,

\begin{equation*}
	\tilde{\phi}(k) = MG\phi(k) + Me(k) + NE\left[\phi(k)\right]
\end{equation*}

Since E[$\tilde{\phi}(k)$] = E[$\phi(k)$] = 0, we have:

\begin{equation*}
\tilde{\phi}(k) = MG\phi(k) + Me(k)
\end{equation*}

Furthermore,

\begin{equation*}
\phi(k) - \tilde{\phi}(k) = (I - MG)\phi(k) - Me(k)
\end{equation*}

Thus, the covariance of the above expression is computed as:

\begin{equation*}
E\left[\left(\phi(k) - \tilde{\phi}(k)\right)\left(\phi(k) - \tilde{\phi}(k)\right)^{T}\right] = E\left[\left((I - MG)\phi(k) - Me(k)\right)\left((I - MG)\phi(k) - Me(k)\right)^{T}\right]
\end{equation*}

On expanding the right side of the equation and taking the error e(k) to be uncorrelated with the wavefront vector $\phi(k)$, and based on the statistical data provided to us, we get the following expression:

\begin{equation*}
E\left[\left(\phi(k) - \tilde{\phi}(k)\right)\left(\phi(k) - \tilde{\phi}(k)\right)^{T}\right] = (I - MG)C_{\phi}(0)(I - MG)^{T} + M\sigma_{e}^{2}IM^{T}
\end{equation*}

On further factorization, using the Schur complement of the factorized version, and the application of the "completion of squares" argument to the resulting equation, we get the optimum value of the matrix M that minimizes the covariance expression as:

\begin{equation*}
	M = C_{\phi}(0)G^{T}(GC_{\phi}(0)G^{T} + \sigma^{2}I)^{-1}
\end{equation*}

(A point to note here: the term in brackets is invertible since $C_{\phi}(0)$ is a positive definite matrix and $\sigma^{2}I$ is non-singular)

Accordingly, we also get the optimum estimate of the wavefront vector,

\begin{equation*}
\hat\phi(k|k) = C_{\phi}(0)G^{T}(GC_{\phi}(0)G^{T} + \sigma_{e}^{2}I)^{-1}s_{o}(k)
\end{equation*}

\paragraph*{}
\textit{For questions 3 to 5, we assume E[$\epsilon$(k)] = 0 and E[$\epsilon(k)\epsilon(k)^T$] = $C_{\phi}(0)$}

\subsection*{Question 3}

Now, we consider the closed-loop system, and proceed to derive a UMVE of $\epsilon(k)$ using the given measurement set s(k).
As in the previous question, we are provided with some prior information about the wavefront, viz.:

\begin{itemize}
	\item E[$\epsilon(k)$] = 0
	\item E$\left[\epsilon(k)\epsilon(k)^T\right]$ = $C_{\phi}(0)$
	\item noise variance = $\sigma_{e}^{2}$
\end{itemize}

We can clearly see that the equations (2) and (5) in the given assignment are similar, and we are told that the statistical data (the wavefront's and noise's mean and covariance values) are the same. Hence, as in the previous question, the optimum estimate of the wavefront vector is quite similar in the closed-loop system, and the only difference is in the value of the output vector, which in this case will be the closed-loop slope vector s(k):

\begin{equation*}
\hat\epsilon(k|k) = C_{\phi}(0)G^{T}(GC_{\phi}(0)G^{T} + \sigma_{e}^{2}I)^{-1}s(k)
\end{equation*}

\subsection*{Question 4}

In this question, we make use of the random walk model represented by equation (9) in the assignment.

We know that:

\begin{equation*}
\begin{aligned}
&\epsilon(k) = \phi(k) - Hu(k-1) \\
&\implies \phi(k) = \epsilon(k) + Hu(k-1) \\
&\implies \phi(k+1) = \epsilon(k+1) + Hu(k)
\end{aligned}
\end{equation*}

From the random walk model, we can relate $\phi(k)$ and $\phi(k+1)$, and substituting the above expressions respectively yields:

\begin{equation*}
\epsilon(k+1) + Hu(k) = \epsilon(k) + Hu(k-1) + \eta(k)
\end{equation*}

If we consider $\hat\epsilon(k|k)$ to be the current optimal estimate, we know that the optimal one-step ahead prediction should not be stochastic in nature, and must be estimated based on the current optimal estimate. Based on the above equation, we can thus say,

\begin{equation}\label{eq:ques4}
\hat\epsilon(k+1|k) = \hat\epsilon(k|k) + Hu(k-1) - Hu(k)
\end{equation}

\subsection*{Question 5}

We denote $\delta u(k) := u(k) - u(k-1)$. We know from the previous question's equation ~\ref{eq:ques4} that:

\begin{equation*}
\hat\epsilon(k+1|k) = \hat\epsilon(k|k) - H\delta u(k)
\end{equation*}

Thus, the minimization problem as described in the assignment's equation (6) can be reformulated as:

\begin{equation*}
	\underset{\delta u(k)}{min} \, \|\hat\epsilon(k|k) - H\delta u(k)\|_{2}^{2}
\end{equation*}

This is clearly a linear least-squares problem, and we know from running the \textit{rank} command on the matrix H that it is full-rank. Hence, we can say that the optimal increment, $\hat\delta u(k)$ for minimizing the 2-norm is:

\begin{equation*}
\hat\delta u(k) = (H^{T}H)^{-1}H^{T}\hat\epsilon(k|k)
\end{equation*}

We computed the expression for $\hat\epsilon(k|k)$ in question 3, and substituting the expression in the above equation yields the value of $\hat\delta u(k)$ as:

\begin{equation*}
\hat\delta u(k) = (H^{T}H)^{-1}H^{T}C_{\phi}(0)G^{T}(GC_{\phi}(0)G^{T} + \sigma_{e}^{2}I)^{-1}s(k)
\end{equation*}

Moreover, since H is invertible, we can further simplify the expression as:

\begin{equation}\label{eq:delta_uk}
\hat\delta u(k) = H^{-1}C_{\phi}(0)G^{T}(GC_{\phi}(0)G^{T} + \sigma_{e}^{2}I)^{-1}s(k)
\end{equation}

\subsection*{Question 6}

Given all the data, we compute the values for u(k) recursively based on the values of u(k-1) as defined by the relation in equation \ref{eq:delta_uk}. We take u(1) to be 0, since we assume that for the first iteration, there's no input actuation applied through the deformable mirror. Accordingly, we get:

\begin{equation*}
\epsilon(1) = \phi_{sim}(1)
\end{equation*}

Moreover, since we're provided with $\phi_{sim}$ values but not the values for the actual slope measurements s(k), we compute s(k) as: 

\begin{equation*}
s(k) = G\epsilon(k) + \sigma_{e}*randn(N_{s},1) 
\end{equation*}

where \texttt{randn()} is a MATLAB function used for generating a normally distributed random noise vector.
This s(k) is used in the computation of the optimum u(k) value.
We iterate over all the time samples to get the u(k) matrix for all input vectors, and once we have these values, we can calculate the $\epsilon$ matrix for all time samples based on: 

\begin{equation*}
\epsilon(k) = \phi(k) - Hu(k-1) 
\end{equation*}

We apply the correction process of subtracting the mean vector from $\epsilon(k)$ and then calculate the variance of the vector. The variance for each time sample is calculated accordingly and the average variance of all time samples is returned by the function \texttt{AOloopRW()}.

\subsection*{Question 7}

We compare the variance values returned by the random-walk model-based control design and the open-loop case (in which no control is applied). The variance returned by the former is around a value of 6.5 whereas the latter's returned value is around 22. We can clearly see a reduction in the variance of $\epsilon(k)$ values after controlling the residual wavefront using the DM.

The VAF is calculated based on another function, \texttt{VAF\_RW} in which the $\phi(k+1|k)$ values are compared against $\phi_{sim}(k+1)$ values (after taking into consideration the mean value subtraction as performed in Question 6). The value returned is around 70\%.

\section*{2. VAR Model}

We are provided with a VAR model represented by:

\begin{equation}\label{eq:var_eqn}
	\phi(k+1) = A\phi(k) + w(k)
\end{equation}

We have the following information provided to us to demonstrate that w(k) is uncorrelated with the measurement noise and the wavefront:
\begin{itemize}
\item $E\left[w(k)e(k)^{T}\right] = 0$
\item $E\left[w(k)\phi(k)^{T}\right] = 0$
\end{itemize}

We are also provided with statistical information about w(k): w(k) $\sim \mathcal{N}(0, C_{w})$

\subsection*{Question 1}

We have the following data:

\begin{itemize}
	\item $C_{\phi}(0) = E\left[\phi(k)\phi(k)^{T}\right]$
	\item $C_{\phi}(1) = E\left[\phi(k+1)\phi(k)^{T}\right]$
\end{itemize}

Multiplying equation \ref{eq:var_eqn} with $\phi(k)^{T}$ on both sides yields:

\begin{equation*}
\phi(k+1)\phi(k)^{T} = A\phi(k)\phi(k)^{T} + w(k)\phi(k)^{T}
\end{equation*}

Further, on taking the expectation:

\begin{equation*}
E\left[\phi(k+1)\phi(k)^{T}\right] = E\left[A\phi(k)\phi(k)^{T}\right] + E\left[w(k)\phi(k)^{T}\right]
\end{equation*}

Thus, we get the relation:

\begin{equation*}
	C_{\phi}(1) = AC_{\phi}(0)
\end{equation*}

and hence we calculate:
\begin{equation*}
	A = C_{\phi}(1)C_{\phi}(0)^{-1}
\end{equation*}


\subsection*{Question 2}

We make an assumption here that $\phi(k)$ is a WSS signal.

Multiplying equation \ref{eq:var_eqn} with $w(k)^{T}$ on both sides, and then taking the resulting expectation yields:

\begin{equation*}
E\left[\phi(k+1)w(k)^{T}\right] = E\left[A\phi(k)w(k)^{T}\right] + E\left[w(k)w(k)^{T}\right]
\end{equation*} 

Based on the data we have, we get the following relation based on the above equation:

\begin{equation*}
	E\left[\phi(k+1)w(k)^{T}\right] = C_{w}
\end{equation*}

Taking the transpose of equation \ref{eq:var_eqn}, and multiplying with $\phi(k+1)$ on both sides, we get: 

\begin{equation*}
E\left[\phi(k+1)\phi(k+1)^{T}\right] = E\left[A\phi(k+1)\phi(k)^{T}\right] + E\left[\phi(k+1)w(k)^{T}\right]
\end{equation*} 

Since we have assumed $\phi(k)$ to be WSS, we can say that:\newline
$E\left[\phi(k+1)\phi(k+1)^{T}\right] = E\left[\phi(k)\phi(k)^{T}\right] = C_{\phi}(0)$ 

And we know from the previous question that $C_{\phi}(1) = AC_{\phi}(0)$\newline
Thus,

\begin{equation*}
\begin{aligned}
C_{\phi}(0) = AC_{\phi}(1) + C_{w}\\
\implies C_{\phi}(0) = A^2C_{\phi}(0) + C_{w}
\end{aligned}
\end{equation*}

So we derive the following relationship:

\begin{equation*}
C_{w} = (I-A^2)C_{\phi}(0)
\end{equation*} 

\subsection*{Question 3}

We need to formulate a state-space model with $\epsilon(k)$ being the state vector and s(k) being the output.

Based on equation (4) from the assignment, we can write:

\begin{equation*}
\begin{aligned}
\phi(k) = \epsilon(k) + Hu(k-1)\\
\implies \phi(k+1) = \epsilon(k+1) + Hu(k)
\end{aligned}
\end{equation*} 

Substituting this expression for $\phi(k)$ in equation (10), we get:

\begin{equation*}
\begin{aligned}
\epsilon(k+1) + Hu(k) = A\epsilon(k) + AHu(k-1) + w(k)\\
\implies \epsilon(k+1) = A\epsilon(k) + AHu(k-1) - Hu(k) + w(k)
\end{aligned}
\end{equation*} 

Combining the above relation with equation (5) from the assignment, we have the following state-space model as required:

\begin{equation*}
\begin{aligned}
\epsilon(k+1) &= A\epsilon(k) + \xi(k) + w(k)\\
s(k) &= G\epsilon(k) + e(k)\\
&where\\
\xi(k) &= AHu(k-1) - Hu(k)
\end{aligned}
\end{equation*} 

\subsection*{Question 4}

Based on the state-space formulation from Question 3, we can express the Kalman filter in the observer form as:

\begin{equation*}
	\hat\epsilon(k+1) = (A - KG)\epsilon(k) + AHu(k-1) - Hu(k) + Ks(k)
\end{equation*}

We make use of the following DARE to compute the covariance matrix of $\epsilon(k)$, P, to which the optimal estimate of $\hat P(k+1|k)$ converges on solving the Kalman filter problem, as k $\rightarrow \infty$:

\begin{equation*}
\begin{aligned}
P &= APA^{T} + Q - (APG^{T})(GPG^{T} + R)^{-1}(APG^{T})^{T}\\
&where:\\
Q &= C_{w}\\
R &= E[v(k)v(k)^{T}]
\end{aligned}
\end{equation*}

An important point to note here is that v(k) and w(k) are taken to be uncorrelated, and the matrices A and G to be time-invariant.

Based on the P matrix calculated on solving the above Riccati equation, we compute the value of the stationary Kalman gain,

\begin{equation*}
	K = APG^{T}(GPG^{T} + R)^{-1}
\end{equation*}

\subsection*{Question 5}

The minimum unbiased variance estimate for a state $x(k+1)$ is given by the following expression, which is derived based on the conditions of the conventional Kalman filtering problem (here, we assume a stationary Kalman filter):

\begin{equation*}
\hat{x}(k+1|k) = Ky(k) + Bu(k) + (A - KC)\hat{x}(k|k-1)
\end{equation*}

Here, x(k) is the state vector, y(k) is the output, and A,B, and C are the state-space matrices. In our case, based on our LTI state-space model with $\epsilon(k)$ being the state vector, s(k) being the output vector, and A and G being the state-space matrices, we compute the expression for the optimal one step ahead prediction recursively as:

\begin{equation}\label{eq:eps_optimal}
	\hat\epsilon(k+1|k) = Ks(k) + AHu(k-1) - Hu(k) + (A - KG)\hat\epsilon(k|k-1)
\end{equation}

\subsection*{Question 6}

Based on the control law represented by equation (6) in the assignment, we need to minimize the value of $\|\hat\epsilon(k+1|k)\|_{2}^{2}$ by controlling the input values u(k). From equation \ref{eq:eps_optimal}, we know that we need to minimize the value of:

\begin{equation}
\|(Ks(k) + (A - KG)\hat\epsilon(k|k-1)) + AHu(k-1) - Hu(k) \|_{2}^{2}
\end{equation}

Let us take $\xi(k) = Hu(k) - AHu(k-1)$

In the above equation, let us denote:

$Y(k) = Ks(k) + (A - KG)\hat\epsilon(k|k-1) + AHu(k-1)$

Hence, our minimization problem becomes:

\begin{equation}
\underset{u(k)}{min} \, \| Y(k) - Hu(k) \|_{2}^{2}
\end{equation}

Knowing H to be full-rank and square, we have the linear least-squares solution,

$\hat{u}(k) = H^{-1}Y(k)$

That is to say,

\begin{equation}
\hat{u}(k) = H^{-1}(Ks(k) + (A - KG)\hat\epsilon(k|k-1)) + H^{-1}AHu(k-1)
\end{equation}

Hence the optimum input vectors can be calculated recursively.

\subsection*{Question 7}

The first function, \texttt{computeKalmanAR()} is used for calculating the values of A, $C_{w}$, and K using expressions derived previously in questions 1,2, and 4 respectively.

The function \texttt{AOloopAR()} is used for calculating the values of u(k) for which the 2-norm of $\epsilon(k+1|k)$ is minimum. It is performed using the expression derived in Question 6, and taking $\hat\epsilon(1|0)$ and u(0) to be 0.

u(k) and $\hat\epsilon(k+1|k)$ for the remaining indices can be calculated based on the recursive expressions for the two quantities. Once we have the u(k) values, the $\epsilon(k)$ values can be calculated as $\phi_{sim}(k) - Hu(k-1)$, and accordingly the average variance value can be calculated, as was done in Question (1.) 6 of the random walk model approach.

The variance value of $\epsilon$ returned by \texttt{AOloopAR()} is around 3.5.

\subsection*{Question 8}

We compare the average variance value obtained in this case as against in the random walk model's. The value is further reduced using this model, and we see that this model outperforms the random walk model in terms of minimizing the variance of the estimates.

We compute the VAF for the turbulence wavefront one step ahead predicted values as in the previous case by comparing the predicted $\hat\phi(k+1|k)$ vectors against the $\phi_{sim}(k+1)$ vectors, and obtain a value of around 83 \%.

\section*{3. Subspace Identification}

We are provided with a turbulence state space model description with a general state vector $x$ and are tasked with finding the Matrices $A_s$, $C_s$ and the Kalman gain $K_s$ of the system in innovation form.

\begin{equation}
\begin{aligned}
x(k+1) &= A_s x(k) + K_s v(k)\\
s(k) &= C_s x(k) + v(k)
\end{aligned}
\end{equation}

We generate the identification data i.e. $s_id$ using the provided $\phi_{id}$ data.

\subsection*{Question 1}
We use the N4SID method to estimate the State matrices and the state sequence. This is to ensure that we get the matrices and the Kalman gain up to the same similarity transformation.
\\
\textbf{Data Equation:}\\
\begin{equation}
Y_{s,s,N} = O_s X_{s,N} + \uptau_{v,s} V_{s,s,N}
\end{equation}
Where $Y$,$X$ and $V$ are the Hankel Matrices and $O_s$ is the observability matrix. $\uptau$ is the Toeplitz matrix for the innovation signal.\\
\textbf{Instrumental Variable:}\\
The instrumental variable for our data equation is chosen as:\\
\begin{equation*}
Z_N = Y_{0,s,N}
\end{equation*}
This $Z_N$ thus eliminates $V_{s,s,N}$ from the data equation and gives:
\begin{equation*}
\underset{N\rightarrow \infty}{Lt} \frac{1}{N} Y_{s,s,N} Y_{o,s,N}^{T} = \underset{N\rightarrow \infty}{Lt} \frac{1}{N} O_{s} X_{s,N} Y_{o,s,N}^{T}
\end{equation*}
\\
We use an RQ factorization of the Instrumental variable for efficiency:\\
\begin{equation*}
\begin{bmatrix}
Y_{0,s,N}\\
Y_{s,s,N}\\
\end{bmatrix}
 = \begin{bmatrix}
 R_{11} & 0\\
 R_{21} & R_{22}\\
 \end{bmatrix}
 \begin{bmatrix}
 Q_1\\
 Q_2\\
 \end{bmatrix}
\end{equation*}
\\
And so,
\begin{equation}
O_s X_{s,N} \approx R_{21} R_{11}^{-1} Z_N
\end{equation}
\\
The rank of the above expression (11) is n.\\
Hence we can use this RQ approximation and take it's SVD:\\
\begin{equation*}
R_{21} R_{11}^{-1} Z_N = U_n \Sigma_n V_n^T
\end{equation*}
State sequence is hence estimated as,
\begin{equation*}
\hat{X}_{s,N} = \Sigma_n^{1/2} V_n^T
\end{equation*}
We now use this state sequence to find our State Matrices by solving the linear least squares problem:
\begin{equation*}
\|
\begin{bmatrix}
\hat{X}_{s+1,N}\\
\hat{Y}_{s,1,N-1}\\
\end{bmatrix}
- \begin{bmatrix}
A_T\\
C_T\\
\end{bmatrix}
\begin{bmatrix}
\hat{X}_{s,N-1}\\
\end{bmatrix}
\|_{2}^2
\end{equation*}
\subsection*{Question 2}


\subsection*{Question 3}

\subsection*{Question 4}

\subsection*{Question 5}

\subsection*{Question 6}


\section*{4. Critical Thinking}

\subsection*{Question 1}

In this question, we make use of a slightly different version of the random walk model presented in Section 1, with the control law requiring the minimization of the residual slopes s(k+1) instead of $\epsilon(k+1)$.

We know from equation (5) of the assignment that:

\begin{equation*}
\hat{s}(k+1|k) = G\hat\epsilon(k+1|k)
\end{equation*}

Writing $\hat\epsilon(k+1|k) = \hat\phi(k+1|k) - Hu(k)$, we obtain the relation:

\begin{equation*}
\hat{s}(k+1|k) = G\hat\phi(k+1|k) - GHu(k)
\end{equation*}

Since we are making use of the random walk model as described by equation (9), we know the relation: $\hat\phi(k+1|k) = \hat\phi(k|k)$

Moreover, we also know from \ref{eq:epsk_k} the estimate for $\hat\epsilon(k|k)$, which can be written as $\hat\phi(k|k) - Hu(k-1)$. Thus, we get the estimate for $\hat\phi(k|k)$ in terms of s(k), u(k), and u(k-1).

Combining the above pieces of information together, we get the following expression for $\hat{s}(k+1|k)$:

\begin{equation*}
\hat s(k+1|k) = G\left( C_{\phi}(0)G^{T}(GC_{\phi}(0)G^{T} + \sigma_{e}^{2}I)^{-1}s(k) + Hu(k-1)\right) - GHu(k)
\end{equation*}

We clearly see that $\underset{u(k)}{min}\|\hat s(k+1|k)\|_{2}^{2}$ is a linear least-squares problem if we know the vector u(k-1).

We see that G*H produces a matrix that is not full-rank, and hence we follow the approach from Section 1's Question 1, in which there is no assumption made on the rank.

We get the optimum value of u(k), 

\begin{equation*}
\hat{u}(k) = V_{1}\Sigma^{-1}U_{1}^{T}G\left( C_{\phi}(0)G^{T}(GC_{\phi}(0)G^{T} + \sigma_{e}^{2}I)^{-1}s(k) + Hu(k-1)\right)
\end{equation*}

Where $U_{1}, V_{1},$ and $\Sigma$ are the matrices obtained on performing an SVD of the G*H matrix considering the linearly independent vectors and their singular values.

We know that u(0) = 0, since we do not apply any actuation initially. Based on this information, we calculate the value for u(1) by solving the above minimization problem. Subsequently, we solve the minimization problems for all u(k) values.

Thus, we calculate the variance of $\epsilon(k)$ based on the values calculated from the above u(k) values and the $\phi_{sim}(k+1)$ values provided to us.


\subsubsection*{Question 2}

The following values are obtained for the different methods:

\begin{itemize}
	\item Random walk model: var($\epsilon$) $\sim$ 6.5, and VAF $\sim$ 70 \%
	\item VAR1 model: var($\epsilon$) $\sim$ 3.5, and VAF $\sim$ 83 \%
	\item Subspace Identification model: var($\epsilon$) $\sim$ 2.1, and VAF $\sim$ 90 \%
	\item Slope-minimizing model: var($\epsilon \sim$ 6.5, and VAF $\sim$ 70 \%
\end{itemize}

The above results follow a trend which is expected, since the models used greatly affect the results obtained.

In the \textbf{random walk models}, since we do not make enough use of the data provided to us and follow an approach in which we assume the wavefront to be changing based on a random turbulence value, we cannot really predict accurately the one step ahead wavefronts and thus end up getting large variance values and low VAF values. Moreover, for both the slope-minimizing and the Question 1 RW model, we get approximately the same values since the underlying model is the same in both cases and the minimization of either the predicted s(k+1) values or $\epsilon(k+1)$ should yield the same results (we do not make use of any further information in the slop-minimizing model and thus cannot expect better results).

In the \textbf{VAR1 model}, we make use of a stationary Kalman filter (constructed using the data available to us) and thus can expect better values of variance and VAF than the random walk models (the observer provides a correction term to the model through the Kalman filter, based on which the predicted values are closer to the actual values). We see that the values we obtain are indeed in line with our expectations.

In the \textbf{subspace identification model}, we make use of all the data we have about the system's output and input data, and construct models of different orders that  best represent the system based on the data. After choosing an order based on training data, validation data is used to test the constructed system. Hence, we clearly see that a lot more data is used in this approach to construct our system model, and given that the validation data is from the same family of datasets, we can expect our constructed system model to be quite accurate when tested with the validation data. Moreover, there is no prior assumption made on the system other than that it is represented by a stochastic state-space model. We clearly see from the results that the subspace identification model performs the best in terms of the performance parameters, as expected.

\subsubsection*{Question 3}

The second unobservable mode is one in which the wavefront happens to lie in the nullspace of the G matrix (that is, if it does, the output slope vector is a zero vector which is not truly indicative of the turbulent wavefront). On performing an \textit{imagesc} run on the null vectors, we get the following image pattern:

%\begin{figure}[!h]
%	\centering
%	\includegraphics[width=0.7\linewidth]{G_nullSpaceVectors}
%	\caption{Imagesc representing the kernel of G}
%	\label{fig:gnullspacevectors}
%\end{figure}

Since we make use of wavefront prediction in our models, if we see the above pattern as the predicted wavefront, we can be cautious and change the input ($\phi_{DM}$) so as to move away from the wavefront that would otherwise cause the slope vector to be falsely 0.

\subsubsection*{Question 4}

%----------------------------------------------- --------------------------------
% REFERENCES
%-------------------------------------------------------------------------------
\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

Anand, U., 2010. The Elusive Free Radicals, \textit{The Clinical Chemist,} [e-journal] Available at:<\url{http://www.clinchem.org/content/56/10/1649.full.pdf}> [Accessed 2 November 2013]
\newline
\newline

Biology Forums, 2012. \textit{Normal glomerulus. Acute glomerulonephritis.} [online] Available at: <\url{http://biology-forums.com/index.php?action=gallery;sa=view;id=9284}> [Accessed 23 October 2013].
\newline
\newline

Budisavljevic, M., Hodge, L., Barber, K., Fulmer, J., Durazo-Arvizu, R., Self, S., Kuhlmann, M., Raymond, J. and Greene, E., 2003. Oxidative stress in the pathogenesis of experimental mesangial proliferative glomerulonephritis, \textit{American Journal of Physiology - Renal Physiology,} 285(6), pp. 1138-1148.
\newline
\newline

Chien, C., Lee, P., Chen, C., Ma, M., Lai, M. and Hsu, S., 2001. De Novo Demonstration and Co-localization of Free-Radical Production and Apoptosis Formation in Rat Kidney Subjected to Ischemia/Reperfusion, \textit{Journal of the American Society of Nephrology,} 12(5), pp. 973-982.
\newline
\newline

Couser, W., 1993. Pathogenesis of glomerulonephritis, \textit{Kidney International Supplements,} 42, pp. 19-26.
\newline
\newline

De Gasparo, M., 2002. Angiotensin II and nitric oxide interaction, \textit{Heart Failure Reviews,} [e-journal] Available at:<\url{http://www.ncbi.nlm.nih.gov/pubmed/12379820}> [Accessed 26 October 2013]
\newline
\newline

Edinburgh Renal Education Pages, 2012. \textit{Glomerulonephritis} [online] Available at: <\url{http://www.edrep.org/pages/textbook/glomerulonephritis.php}> [Accessed 25 October 2013].
\newline
\newline

Forbes, J., Coughlan, M. and Cooper, M., 2008. Oxidative Stress as a Major Culprit in Kidney Disease in Diabetes, \textit{Diabetes,} 57(6), pp. 1446-1454.
\newline
\newline

Geeky Medics, 2010. \textit{Glomerulonephritis} [online] Available at: <\url{http://geekymedics.com/2010/10/27/glomerulonephritis/}> [Accessed 25 October 2013].
\newline
\newline

Gryglewski, R., Palmer, R., Moncada, S., 1986. Superoxide anion is involved in the break­down of endothelium derived relaxing factor, \textit{Nature,} 320, pp. 454-456.
\newline
\newline

Halliwell, B., 2001. Free Radicals and other reactive species in Disease, \textit{Encyclopedia of Life Sciences,} [e-journal] Available at:<\url{http://web.sls.hw.ac.uk/teaching/level4/bcm1_2/reading/oxidative_stress/files/Oxidative_stress.pdf}> [Accessed 19 October 2013]
\newline
\newline

Huang, H., Patel, P. and Salahudeen, A., 2001. Lazaroid compounds prevent early but not late stages of oxidant-induced cell injury: potential explanation for the lack of efficacy of lazaroids in clinical trials, \textit{Pharmacological Research,} 41(1), pp. 55-61.
\newline
\newline

Klinger, J., Abman, S. and Gladwin, M., 2013. Nitric Oxide Deficiency and Endothelial Dysfunction in Pulmonary Arterial Hypertension, \textit{American Journal of Respiratory and Critical Care Medicine,} 188(6), pp. 639-646.
\newline
\newline

Lindemann, I., Boettcher, J., Oertel, K., Pasternack, R., Heine, A. and Klebe, G. 2012. Inhibitors of Transglutaminase 2: A therapeutic option in celiac disease, \textit{To be Published,} [e-journal + PDB structure] Available at:<\url{http://www.ebi.ac.uk/pdbe-srv/view/entry/3s3s/summary}> [Accessed 24 October 2013]
\newline
\newline

Mayo Clinic, 2011. \textit{Glomerulonephritis} [online] Available at: <\url{http://www.mayoclinic.com/health/glomerulonephritis/DS00503/}> [Accessed 20 October 2013].
\newline
\newline

McCord, J., Roy, R. and Schaffer, S., 1985. Free radicals and myocardial ischemia. The role of xanthine oxidase, \textit{Advances in myocardiology,} [e-journal] Available at:<\url{http://www.ncbi.nlm.nih.gov/pubmed/2982206}> [Accessed 24 October 2013]
\newline
\newline

National Health Service, 2012. \textit{Causes of glomerulonephritis} [online] Available at: <\url{http://www.nhs.uk/Conditions/Glomerulonephritis/Pages/Causes.aspx}> [Accessed 20 October 2013].
\newline
\newline

Niaudet, P., 2013. \textit{Overview of the pathogenesis and causes of glomerulonephritis in children.} [online] Available at: <\url{http://www.uptodate.com/contents/overview-of- \ the-pathogenesis-and-causes-of-glomerulonephritis-in-children}> [Accessed 21 October 2013].
\newline
\newline

Ronco, P., 2013. \textit{Mechanisms of glomerular crescent formation.} [online] Available at: <\url{http://www.uptodate.com/contents/mechanisms-of-glomerular-crescent-formation}> [Accessed 21 October 2013].
\newline
\newline

Rutchik, J., 2013. \textit{Toxic Neuropathy Clinical Presentation.} [online] Available at: <\url{http://emedicine.medscape.com/article/1175276-clinical#a0216}> [Accessed 26 October 2013].
\newline
\newline

R\&D Systems, 2013. \textit{Technical Information. Ischemia/Reperfusion Injury.} [online] Available at: <\url{http://www.rndsystems.com/cb_detail_objectname_SP96_Ischemia.aspx}> [Accessed 28 October 2013].
\newline
\newline

Salahudeen, A., 1999. Free Radicals in Kidney Disease and Transplantation, \textit{Saudi Journal of Kidney Diseases and Transplantation,} 10(2), pp. 137-143.
\newline
\newline

Sarma, A., Mallick, A. and Ghosh, A., 2010. Free Radicals and Their Role in Different Clinical Conditions: An Overview, \textit{International Journal of Pharma Sciences and Research,} 1(3), pp. 182-192.
\newline
\newline

Shah, S., Baliga, R., Rajapurkar, M. and Fonseca, V., 2007. Oxidants in Chronic Kidney Disease, \textit{Journal of the American Society of Nephrology,} 18(1), pp. 16-28.
\newline
\newline

The University of Utah, Unknown. \textit{Glomerulonephritis} [online] Available at: <\url{http://library.med.utah.edu/WebPath/RENAHTML/RENALIDX.html#8}> [Accessed 25 October 2013].
\newline
\newline

Wang, C. and Salahudeen, A., 1994. Cyclosporine nephrotoxicity: attenuation by an antioxidant -inhibitor of lipid peroxidation in-vitro and in-vivo, \textit{Transplantation,} 58, pp. 940-946.
\newline
\newline

Wang, C. and Salahudeen, A., 1995. Lipid peroxidation accompanies cyclosporine nephrotoxicity: effects of vitamin E, \textit{Kidney International,} 47, pp. 927-934.
\newline
\newline

Weiss, S., 1989. Tissue Destruction by Neutrophils, \textit{New England Journal of Medicine,} 320, pp. 365-376.
\newline
\newline


\end{document}

%-------------------------------------------------------------------------------
% SNIPPETS
%-------------------------------------------------------------------------------

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{file_name}
%	\caption{}
%	\centering
%	\label{label:file_name}
%\end{figure}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{graph}
%	\caption{Blood pressure ranges and associated level of hypertension (American Heart Association, 2013).}
%	\centering
%	\label{label:graph}
%\end{figure}

%\begin{wrapfigure}{r}{0.30\textwidth}
%	\vspace{-40pt}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{file_name}
%	\end{center}
%	\vspace{-20pt}
%	\caption{}
%	\label{label:file_name}
%\end{wrapfigure}

%\begin{wrapfigure}{r}{0.45\textwidth}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{manometer}
%	\end{center}
%	\caption{Aneroid sphygmomanometer with stethoscope (Medicalexpo, 2012).}
%	\label{label:manometer}
%\end{wrapfigure}

%\begin{table}[!ht]\footnotesize
%	\centering
%	\begin{tabular}{cccccc}
%	\toprule
%	\multicolumn{2}{c} {Pearson's correlation test} & \multicolumn{4}{c} {Independent t-test} \\
%	\midrule	
%	\multicolumn{2}{c} {Gender} & \multicolumn{2}{c} {Activity level} & \multicolumn{2}{c} {Gender} \\
%	\midrule
%	Males & Females & 1st level & 6th level & Males & Females \\
%	\midrule
%	\multicolumn{2}{c} {BMI vs. SP} & \multicolumn{2}{c} {Systolic pressure} & \multicolumn{2}{c} {Systolic Pressure} \\
%	\multicolumn{2}{c} {BMI vs. DP} & \multicolumn{2}{c} {Diastolic pressure} & \multicolumn{2}{c} {Diastolic pressure} \\
%	\multicolumn{2}{c} {BMI vs. MAP} & \multicolumn{2}{c} {MAP} & \multicolumn{2}{c} {MAP} \\
%	\multicolumn{2}{c} {W:H ratio vs. SP} & \multicolumn{2}{c} {BMI} & \multicolumn{2}{c} {BMI} \\
%	\multicolumn{2}{c} {W:H ratio vs. DP} & \multicolumn{2}{c} {W:H ratio} & \multicolumn{2}{c} {W:H ratio} \\
%	\multicolumn{2}{c} {W:H ratio vs. MAP} & \multicolumn{2}{c} {\% Body fat} & \multicolumn{2}{c} {\% Body fat} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Height} & \multicolumn{2}{c} {Height} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Weight} & \multicolumn{2}{c} {Weight} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Heart rate} & \multicolumn{2}{c} {Heart rate} \\
%	\bottomrule
%	\end{tabular}
%	\caption{Parameters that were analysed and related statistical test performed for current study. BMI - body mass index; SP - systolic pressure; DP - diastolic pressure; MAP - mean arterial pressure; W:H ratio - waist to hip ratio.}
%	\label{label:tests}
%\end{table}